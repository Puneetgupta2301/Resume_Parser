# -*- coding: utf-8 -*-
"""Resume Parser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BKHBFfF4c5mMhEUSeC-_BHblRa9Jzl3C
"""

import nltk
nltk.download('stopwords')
nltk.download('punkt')

# !pip install docx2txt
# !pip install chart_studio

import warnings
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


from nltk.corpus import stopwords
import string

from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer

import docx2txt
from nltk.tokenize import WhitespaceTokenizer

import plotly.graph_objects as go
import plotly.express as px

import chart_studio.plotly as py

warnings.filterwarnings('ignore')

df = pd.read_csv('UpdatedResumeDataSet.csv', encoding='utf-8')
df.head()

df.head()

df.describe()

df.info()

df.shape

df.dtypes

df[df.isna().any(axis=1) | df.isnull().any(axis=1)]

df['length'] = df['Resume'].str.len()
df['length'].describe()

plt.figure(figsize=(12.8,6))
sns.distplot(df['length']).set_title('Resume length distribution')

"""### Displaying the distinct categories of resume and the number of records belonging to each category"""

df['Category'].value_counts()

plt.figure(figsize=(5,5))
plt.xticks(rotation=90)
sns.countplot(y="Category", data=df, palette='Reds')

df['Category'].value_counts()[:3].index

"""Processing Text

"""

resumeDataSet = df.copy()
resumeDataSet['cleaned_resume'] = ''
resumeDataSet.head()

import re

def cleanResume(resumeText):
    resumeText = re.sub('http\S+\s*', ' ', resumeText)  # remove URLs
    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc
    resumeText = re.sub('#\S+', '', resumeText)  # remove hashtags
    resumeText = re.sub('@\S+', '  ', resumeText)  # remove mentions
    resumeText = re.sub('[%s]' % re.escape("""!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~"""), ' ', resumeText)  # remove punctuations
    resumeText = re.sub(r'[^\x00-\x7f]',r' ', resumeText)
    resumeText = re.sub('\s+', ' ', resumeText)  # remove extra whitespace
    return resumeText

resumeDataSet['cleaned_resume'] = resumeDataSet.Resume.apply(lambda x: cleanResume(x))

"""### Encoding labels into different values"""

var_mod = ['Category']
le = LabelEncoder()
for i in var_mod:
    resumeDataSet[i] = le.fit_transform(resumeDataSet[i])

resumeDataSet.head()

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

requiredText = resumeDataSet['cleaned_resume'].values
requiredTarget = resumeDataSet['Category'].values

word_vectorizer = TfidfVectorizer(
    sublinear_tf=True,
    stop_words='english',
    max_features=1500)
word_vectorizer.fit(requiredText)
WordFeatures = word_vectorizer.transform(requiredText)

print ("Feature completed .....")

X_train,X_test,y_train,y_test = train_test_split(WordFeatures,requiredTarget,random_state=0, test_size=0.2)
print(X_train.shape)

clf = KNeighborsClassifier(n_neighbors=15)
clf = clf.fit(X_train, y_train)
yp = clf.predict(X_test)
print('Accuracy of KNeighbors Classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))
print('Accuracy of KNeighbors Classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))

# !pip install pickle
# import pickle

# save_label_encoder = open("pickles/le.pickle","wb")
# pickle.dump(le, save_label_encoder)
# save_label_encoder.close()

# save_word_vectorizer = open("pickles/word_vectorizer.pickle","wb")
# pickle.dump(word_vectorizer, save_word_vectorizer)
# save_word_vectorizer.close()

# save_classifier = open("pickles/clf.pickle","wb")
# pickle.dump(clf, save_classifier)
# save_classifier.close()

class JobPredictor:
    def __init__(self) -> None:
        self.le = le
        self.word_vectorizer = word_vectorizer
        self.clf = clf

    def predict(self, resume):
        feature = self.word_vectorizer.transform([resume])
        predicted = self.clf.predict(feature)
        resume_position = self.le.inverse_transform(predicted)[0]
        return resume_position

    def predict_proba(self, resume):
        feature = self.word_vectorizer.transform([resume])
        predicted_prob = self.clf.predict_proba(feature)
        return predicted_prob[0]

job_description = """
Job Overview

We are looking for a passionate Data Science Intern to join our analytics team for a 6-month internship. As an intern, you will support data-driven projects by analyzing datasets, building basic machine learning models, and creating visualizations. This role is ideal for students or recent graduates eager to gain hands-on experience in data science and contribute to real-world business solutions.

Responsibilities





Assist in collecting, cleaning, and preprocessing datasets for analysis.



Perform exploratory data analysis to identify trends and insights.



Develop and test simple machine learning models under supervision.



Create data visualizations to communicate findings to team members.



Support senior data scientists in deploying models and interpreting results.



Document processes and findings for project reports.

Required Skills





Programming: Basic knowledge of Python or R for data analysis.



Machine Learning: Familiarity with scikit-learn or similar libraries (e.g., basic regression or clustering).



Data Manipulation: Understanding of Pandas or SQL for data handling.



Visualization: Exposure to Matplotlib, Seaborn, or similar tools.



Statistics: Basic understanding of statistical concepts (e.g., mean, standard deviation, correlation).



Learning Mindset: Eagerness to learn and apply data science techniques.

Qualifications





Current enrollment in or recent graduation from a Bachelor’s program in Computer Science, Statistics, Mathematics, or a related field.



Completion of at least one academic project or coursework in data science or machine learning.



Strong problem-solving skills and attention to detail.



Ability to work collaboratively in a team environment.

Preferred Qualifications





Familiarity with Jupyter Notebooks or data science workflows.



Basic knowledge of databases or data querying.



Interest in business applications of data science.
"""

resume_position = JobPredictor().predict(job_description)
f'JD uploaded! Position: {resume_position}'

"""Cosine Similarity"""



text_tokenizer= WhitespaceTokenizer()
remove_characters= str.maketrans("", "", "±§!@#$%^&*()-_=+[]}{;'\:,./<>?|")
cv = CountVectorizer()

resume_docx = docx2txt.process('RhythemJainResume.docx')

#takes the texts in a list
text_docx= [resume_docx, job_description]
#creating the list of words from the word document
words_docx_list = text_tokenizer.tokenize(resume_docx)
#removing speacial charcters from the tokenized words
words_docx_list=[s.translate(remove_characters) for s in words_docx_list]
#giving vectors to the words
count_docx = cv.fit_transform(text_docx)
#using the alogorithm, finding the match between the resume/cv and job description
similarity_score_docx = cosine_similarity(count_docx)
match_percentage_docx= round((similarity_score_docx[0][1]*100),2)
f'Match percentage with the Job description: {match_percentage_docx}'

